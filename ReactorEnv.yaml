seed: 42
num_of_seeds: 1
process_name: 'ReactorEnv'
normalize: False
dense_reward: True
debug_mode: False

# for online learning
online_training: False
buffer_maxlen: 1000000
explorer_start_epsilon: 1.0
explorer_end_epsilon: 0.1
explorer_duration: 20000
n_steps_per_epoch: 200
online_random_steps: 600
online_update_interval: 100
online_save_interval: 100

scheduler_name: 'fifo_scheduler'
time_budget_s: 86400 # a day in seconds
use_tune: True
num_gpus: 1
num_workers: 1
# num_gpus_per_worker: 0.1
train_iter: 2000
online_alg: 'ppo'
online_logs_location: "ray"
online_plt_dir: "plt_results"
online_resylts: "ray_results"
log_to_file: 'logfile.log'

# for offline data generation and training
N_EPOCHS: 100
DYNAMICS_N_EPOCHS: 100
scaler: 'min_max' # None, 'pixel', 'min_max', 'standard'
action_scaler: 'min_max' # None, 'min_max'
reward_scaler: 'min_max' # None, 'min_max', 'standard'
evaluate_on_environment: True
logs_location: "d3rlpy_logs/"
plt_dir: "plt"
dataset_location: datasets 
training_dataset: 'train_1000_normalize=False.pkl'
eval_dataset: 'test_1000_normalize=False.pkl'
test_initial_states: 'initial_states_step_10.npy'
training_num_of_initial_state: 1000
eval_num_of_initial_state: 1000


# env specific configs
reward_on_steady: True
reward_on_absolute_efactor: False
compute_diffs_on_reward: True
standard_reward_style: 'setpoint'
initial_state_deviation_ratio: 0.1
