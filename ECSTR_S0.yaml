seed: 42
num_of_seeds: 1
model_name: 'ppo'
normalize: False
dense_reward: True
debug_mode: False
process_name: 'ECSTR_S0'

# for online learning
online_training: False
buffer_maxlen: 1000000
explorer_start_epsilon: 1.0
explorer_end_epsilon: 0.1
explorer_duration: 20000
n_steps_per_epoch: 200
online_random_steps: 600
online_update_interval: 100
online_save_interval: 100

# for offline data generation and training
N_EPOCHS: 100
DYNAMICS_N_EPOCHS: 100
scaler: 'min_max' # None, 'pixel', 'min_max', 'standard'
action_scaler: 'min_max' # None, 'min_max'
reward_scaler: 'min_max' # None, 'min_max', 'standard'
evaluate_on_environment: True
default_loc: "d3rlpy_logs/"
plt_dir: "plt_results"
dataset_location: datasets_ECSTR_S0 
training_dataset_loc: 'datasets_ECSTR_S0/pid_step_50_normalize=False.pkl'
eval_dataset_loc: 'datasets_ECSTR_S0/pid_step_10_normalize=False.pkl'
test_initial_states: 'datasets_ECSTR_S0/initial_states_step_10.npy'
generate_initial_states: True
load_full_states: True
npseparate_initial_states: False
generate_algorithmatic_dataset: True
num_of_initial_state: 1000

# env specific configs
reward_on_steady: True
reward_on_absolute_efactor: False
compute_diffs_on_reward: True
standard_reward_style: 'setpoint'
initial_state_deviation_ratio: 0.15
